# 자연어 처리 NLTK 패키지

## Corpus (코퍼스)

- 자연어 처리를 위해 (기계) 학습용 기초 데이터를 corpus라 한다.
  - Gutenberg corpus
  - Brown corpus 등 다양한 코퍼스가 있다.

## Part of speech tagging (POS tagging) 품사 태깅

- 품사 종류: (한국어의 경우) 명사, 대명사, 수사, 조사, 동사, 형용사, 관형사, 감탄사.
- 문장에 사용된 단어들에 알맞는 품사를 결정하는 것을 품사 태깅 이라 한다.
- 문장의 구조가 애매한 경우 (ambiguity)에는 100% 올바르게 품사를 태깅하기는 어렵다. 예를 들어, I can can. 의 경우와 I can can a can. 같은 문장. 
- 현재 품사 태깅의 정확도는 약 94% ~ 98% 정도로 알려져 있다. 
- 품사 태깅의 목적은 문장의 의미 (semantic)를 파악하거나, 문법 (syntax)에 맞는 문장을 생성하기 위해서다. 
- 기계 번역 (machine translation) 같은 경우, 문장의 의미를 파악한 후 다른 언어의 구조에 맞게 다른 문장을 생성한다. 이 때 품사 태깅이 필요하다. 
- 챗봇같은 경우도 화자가 말한 문장의 의미를 파악해야 하고, 어법에 맞는 답변을 해야 하기 때문에 품사 태깅이 필요하다.

### 히든 마코프 모델 (HMM)

![스크린샷 2020-07-31 오후 2.29.42](스크린샷 2020-07-31 오후 2.29.42-6176526.png)

## ![스크린샷 2020-07-31 오후 2.30.06](스크린샷 2020-07-31 오후 2.30.06.png)

![스크린샷 2020-07-31 오후 2.31.46](스크린샷 2020-07-31 오후 2.31.46.png)

## 문서 분류 (Document Classificaiton)

- 학습 문서에 부여된 label을 학습하여 일반 문서의 label을 추정할 때 문서 분류 기술을 이용한다. 
- 문서 분류 알고리즘은 일반적으로 기계학습이나 딥러닝을 이용한다. 
- 문서 분류는 SNS 등 문서의 감성분석 (sensitivity analysis), 문서의 주제 파악, 카테고리별 문서 분류, POS 태깅 등에 널리 사용된다.

![스크린샷 2020-07-31 오후 2.33.58](스크린샷 2020-07-31 오후 2.33.58.png)

- 학습 데이터가 왼쪽 그림처럼 구성된 경우 분류를 위한 경계선은 오른쪽의 트리 형식으로 표현할 수 있다. 
- 아래 예시는 입력 변수가 2개 (x, y) 인 경우이며 2차원 평면상에 배치된 데이터들을 4개의 직선으로 분류한 경우이다. 
- 입력 변수가 3개 이상인 다차원 구조에서는 직선 대신 (초) 평면으로 구분된다. 
- 트리 구조로 경계선을 구분하면 차원이 높아져도 직관적으로 이해하기 쉬우므로, 분류 기법으로 많이 사용된다. 
- 경계선을 너무 많이 사용하면 트리 구조가 복잡해지고, 과잉 적합의 우려가 있다. 
- 과잉 적합 문제를 해결하기 위한 방편으로 트리가 너무 복잡해지지 않도록 사전/사후 가지치기 (Pruning) 방법이 사용된다. 
- 일반화 특성과 정확도를 향상시키기 위해서는 훈련 데이터를 여러 개로 분할하고, 분할된 데이터 마다 트리를 구성한 후 결과를 종합하는 랜덤 포레스트 (Random Forest) 방법이 사용된다.

![스크린샷 2020-07-31 오후 2.36.02](스크린샷 2020-07-31 오후 2.36.02.png)

### 앙상블 기법 (Ensemble or Classifier Combination) -- 성능 개선 방법: 랜덤  포레스트 (Random Forest)

- 랜덤 포레스트는 Decision Tree를 이용한 앙상블 기법이다. 
- 랜덤 포레스트는 다수의 Decision Tree의 예측 결과를 종합하여 분류한다. 
- 배깅과 동일하게 훈련 데이터에서 n-개의 서브 데이터를 샘플링하고 (중복 허용, 균등 분포), 각 서브 데이터마다 Decision Tree를 구축한다. 
- 서브 데이터는 훈련 데이터로부터 랜덤하게 추출되므로, 서브 트리들은 약간씩 다르게 구성된다. 
- 약간씩 다른 트리의 결론을 종합하므로 분산이 감소하고 일반화 특성이 향상된다



![스크린샷 2020-07-31 오후 2.57.31](스크린샷 2020-07-31 오후 2.57.31.png)

![스크린샷 2020-07-31 오후 2.57.47](스크린샷 2020-07-31 오후 2.57.47.png)

